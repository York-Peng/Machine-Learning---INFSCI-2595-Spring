---
title: "Part_ii-A"
author: "Peng Yuan"
date: '2023-04-12'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Part ii: Regression - iiA) Linear models
### 1. Load package
```{r}
library(tidyverse)
library(caret)
library(coefplot)
library(splines)
library(coefplot)
library(yardstick)
```

### 2. Read data and LOGIT-transformed response
#### 2.1 Read Csv
```{r,2.1_read_data}
df <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)
```

**LOGIT-transform data**
```{r,2.2_Logit_data}
dfii_ready <- df %>% 
  mutate(y = boot::logit( (response - 0) / (100 - 0) ) ) %>% 
  subset(select = c(R, G, B, 
         Lightness, Saturation, Hue,response,
         y))


dfii <- dfii_ready %>%
  subset(select = c(R, G, B, Hue)) %>% 
  scale() %>% as.data.frame() %>%
  bind_cols(dfii_ready %>% subset(select = c(Lightness, Saturation, y)))


dfii %>% glimpse()
```

### 3.Build linear model
```{r}
#Intercept-only model – no INPUTS!
mod1 <- lm(y ~ 1 , dfii)
```

```{r,}
#Categorical variables only – linear additive
mod2 <- lm(y ~ as.factor(Lightness) + as.factor(Saturation),dfii)
```


```{r}
#Continuous variables only – linear additive
mod3 <- lm(y ~ R + G + B + Hue, dfii)
```

```{r}
#All categorical and continuous variables – linear additive
mod4 <- lm(y ~ R + G + B + Hue + Saturation + Lightness, dfii)
```

```{r}
#Interaction of the categorical inputs with all continuous inputs main effects
mod5 <- lm(y ~ (R + G + B + Hue) * (Saturation + Lightness), dfii)
```

```{r}
#Add categorical inputs to all main effect and all pairwise interactions of continuous inputs
mod6 <- lm(y ~ (R + G + B + Hue)^2 + (Saturation + Lightness), data = dfii)
```

```{r}
#Interaction of the categorical inputs with all main effect and all pairwise interactions of continuous inputs
mod7 <- lm(y ~ (R + G + B + Hue)^2 + (Saturation * Lightness), dfii)
```

```{r}
#Try non-linear basis functions based on your EDA.
mod8 <- lm(y ~ (I(R^2) + I(G^2) + I(B^2) + I(Hue^2)) + (Saturation + Lightness), dfii)
```

```{r}
#Can consider interactions of basis functions with the categorical inputs!
#Can consider interactions of basis functions with other basis functions!
mod9 <- lm(y ~ (I(R^2) + I(G^2) + I(B^2) + I(Hue^2)) * (Saturation + Lightness), dfii)
```

```{r}
mod10 <- lm(y ~ (I(R^2) + I(G^2) + I(B^2) + I(Hue^2) + R * G * B * Hue) + (Saturation * Lightness), dfii)
```

### 4. Model Evaluation
```{r}
p1 <- broom::glance(mod1)
p2 <- broom::glance(mod2)
p3 <- broom::glance(mod3)
p4 <- broom::glance(mod4)
p5 <- broom::glance(mod5)
p6 <- broom::glance(mod6)
p7 <- broom::glance(mod7)
p8 <- broom::glance(mod8)
p9 <- broom::glance(mod9)
p10 <- broom::glance(mod10)

p_all <- rbind(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10) %>%
  tibble::rowid_to_column()
```

#### 4.1 R-squared
```{r,4.1_solution}
p_all %>% ggplot(mapping = aes(x = rowid, y = r.squared)) +
  geom_path()+
  geom_point(size = 2.0)+
  labs(x = '')+
  theme_bw()
```
#### 4.2 AIC
```{r,4.2_solution}
p_all %>% ggplot(mapping = aes(x = rowid, y = AIC)) +
  geom_path()+
  geom_point(size = 2.0)+
  labs(x = '')+
  theme_bw()
```

#### 4.3 BIC
```{r,4.3_solution}
p_all %>% ggplot(mapping = aes(x = rowid, y = BIC)) +
  geom_path()+
  geom_point(size = 2.0)+
  labs(x = '')+
  theme_bw()
```

**I would like to choose the models with smaller BIC and AIC and larger R- square. In AIC result, Model 10 is the lowest and Model 5 is the second lowest. In the BIC result, Model 10 is the lowest and the Model 5 is the second lowest.Thus, Model 5, Model 9 and Model 10 are the top 3 model. I will use these three models in the following part.**

### 5. How do the coefficient summaries compare between the top 3 models?
#### 5.1 Model 5
```{r,model_5}
mod5 %>%
  coefplot(scales = "free", sort = "magnitude", pointSize = 1, innerCI = 0.5)+
  geom_vline(xintercept = 0, color = "red")+
  theme_bw()+
  theme(legend.position = 'none')
```

```{r}
broom::tidy(mod5) %>% filter(p.value < 0.05) %>%
  arrange(desc(abs(estimate)))
```


There are 40 significant coefficient features in Model 5. G is the most important features.

The variables printed above are important variables and are printed in ascending order of importance.


#### 5.2 Model 9
```{r,5.2_model_9}
mod9 %>%
  coefplot(scales = "free", sort = "magnitude", pointSize = 1, innerCI = 0.5)+
  geom_vline(xintercept = 0, color = "red")+
  theme_bw()+
  theme(legend.position = 'none')
```

```{r}
broom::tidy(mod9) %>% filter(p.value < 0.05) %>%
  arrange(desc(abs(estimate)))
```

There are 31 significant coefficient features in Model 9. The lightness-pale is the most important features.

The variables printed above are important variables and are printed in ascending order of importance.

#### 5.3 Model 10
```{r,5.2_model_10}
mod10 %>%
  coefplot(scales = "free", sort = "magnitude", pointSize = 1, innerCI = 0.5)+
  geom_vline(xintercept = 0, color = "red")+
  theme_bw()+
  theme(legend.position = 'none')
```

```{r}
broom::tidy(mod10) %>% filter(p.value < 0.05) %>%
  arrange(desc(abs(estimate)))
```

There are 17 significant coefficient features in Model 10. G is the most important features.

The variables printed above are important variables and are printed in ascending order of importance.

**Although in the later steps I divided the data into train and test set and calculated the RMSE and R2, they were of no practical significance for the selection of the model. In this part, we do not have stable standard deviations and \miu, so the model is not chosen by following the RMSE and R2**



### Train Model
Slice the train and test sets
```{r}
trainIndex <- sample(1:nrow(dfii), round(nrow(dfii)*0.2))

# Split data into training and test sets
train <- dfii[trainIndex, ]
test <- dfii[-trainIndex, ]
```

train model
```{r}
#Intercept-only model – no INPUTS!
mod1_lm <- lm(y ~ 1 , train)

#Categorical variables only – linear additive
mod2_lm <- lm(y ~ as.factor(Lightness) + as.factor(Saturation),train)

#Continuous variables only – linear additive
mod3_lm <- lm(y ~ R + G + B + Hue, train)

#All categorical and continuous variables – linear additive
mod4_lm <- lm(y ~ ., train)

#Interaction of the categorical inputs with all continuous inputs main effects
mod5_lm <- lm(y ~ (R + G + B + Hue) * (Saturation + Lightness), train)

#Add categorical inputs to all main effect and all pairwise interactions of continuous inputs
mod6_lm <- lm(y ~ (R + G + B + Hue)^2 + (Saturation + Lightness), data = train)


#Interaction of the categorical inputs with all main effect and all pairwise interactions of continuous inputs
mod7_lm <- lm(y ~ (R + G + B + Hue)^2 + (Saturation * Lightness), train)

#Try non-linear basis functions based on your EDA.
#Can consider interactions of basis functions with other basis functions!
#Can consider interactions of basis functions with the categorical inputs!

mod8_lm <- lm(y ~ (I(R^2) + I(G^2) + I(B^2) + I(Hue^2)) + (Saturation + Lightness), train)

mod9_lm <- lm(y ~ (I(R^2) + I(G^2) + I(B^2) + I(Hue^2) + R * G * B * Hue) * (Saturation + Lightness), train)

mod10_lm <- lm(y ~ (I(R^2) + I(G^2) + I(B^2) + I(Hue^2) + R * G * B * Hue) + (Saturation * Lightness), train)
```


```{r}
pred_train_mod1 <- as.vector(mod1_lm$fitted.values)
pred_train_mod2<- as.vector(mod2_lm$fitted.values)
pred_train_mod3 <- as.vector(mod3_lm$fitted.values)
pred_train_mod4 <- as.vector(mod4_lm$fitted.values)
pred_train_mod5 <- as.vector(mod5_lm$fitted.values)
pred_train_mod6 <- as.vector(mod6_lm$fitted.values)
pred_train_mod7 <- as.vector(mod7_lm$fitted.values)
pred_train_mod8 <- as.vector(mod8_lm$fitted.values)
pred_train_mod9 <- as.vector(mod9_lm$fitted.values)
pred_train_mod10 <- as.vector(mod10_lm$fitted.values)
```

```{r}
y_train <- train %>% subset(select = y) %>% pull()

train_metrics <- function(y_train,pred_train_mod){
  tibble::tibble(
    rmse_value = rmse_vec(y_train, pred_train_mod),
    mae_value = mae_vec(y_train, pred_train_mod),
    r2_value = rsq_vec(y_train, pred_train_mod)
  )
}

train_metrics_mod1 <- train_metrics(y_train,pred_train_mod1)
train_metrics_mod2 <- train_metrics(y_train,pred_train_mod2)
train_metrics_mod3 <- train_metrics(y_train,pred_train_mod3)
train_metrics_mod4 <- train_metrics(y_train,pred_train_mod4)
train_metrics_mod5 <- train_metrics(y_train,pred_train_mod5)
train_metrics_mod6 <- train_metrics(y_train,pred_train_mod6)
train_metrics_mod7 <- train_metrics(y_train,pred_train_mod7)
train_metrics_mod8 <- train_metrics(y_train,pred_train_mod8)
train_metrics_mod9 <- train_metrics(y_train,pred_train_mod9)
train_metrics_mod10 <- train_metrics(y_train,pred_train_mod10)
```



```{r}
pred_test_mod1 <- as.vector(predict(mod1_lm, newdata = test))
pred_test_mod2 <- as.vector(predict(mod2_lm, newdata = test))
pred_test_mod3 <- as.vector(predict(mod3_lm, newdata = test))
pred_test_mod4 <- as.vector(predict(mod4_lm, newdata = test))
pred_test_mod5 <- as.vector(predict(mod5_lm, newdata = test))
pred_test_mod6 <- as.vector(predict(mod6_lm, newdata = test))
pred_test_mod7 <- as.vector(predict(mod7_lm, newdata = test))
pred_test_mod8 <- as.vector(predict(mod8_lm, newdata = test))
pred_test_mod9 <- as.vector(predict(mod9_lm, newdata = test))
pred_test_mod10 <- as.vector(predict(mod10_lm, newdata = test))
```


```{r}
y_test <- test %>% subset(select = y) %>% pull()

test_metrics <- function(y_test,pred_test_mod){
  tibble::tibble(
    rmse_value = rmse_vec(y_test, pred_test_mod),
    mae_value = mae_vec(y_test, pred_test_mod),
    r2_value = rsq_vec(y_test, pred_test_mod)
  )
}

test_metrics_mod1 <- test_metrics(y_test,pred_test_mod1)
test_metrics_mod2 <- test_metrics(y_test,pred_test_mod2)
test_metrics_mod3 <- test_metrics(y_test,pred_test_mod3)
test_metrics_mod4 <- test_metrics(y_test,pred_test_mod4)
test_metrics_mod5 <- test_metrics(y_test,pred_test_mod5)
test_metrics_mod6 <- test_metrics(y_test,pred_test_mod6)
test_metrics_mod7 <- test_metrics(y_test,pred_test_mod7)
test_metrics_mod8 <- test_metrics(y_test,pred_test_mod8)
test_metrics_mod9 <- test_metrics(y_test,pred_test_mod9)
test_metrics_mod10 <- test_metrics(y_test,pred_test_mod10)
```

```{r}
train_metrics_all <- bind_rows(train_metrics_mod1,train_metrics_mod2,train_metrics_mod3,train_metrics_mod4,train_metrics_mod5,train_metrics_mod6,
                              train_metrics_mod7,train_metrics_mod8,train_metrics_mod9,train_metrics_mod10)
train_metrics_all <- train_metrics_all %>% mutate(model_number = 1:10,
                                                  type = "train",
                                                  ID = 1:10)
train_metrics_all


test_metrics_all <- bind_rows(test_metrics_mod1,test_metrics_mod2,test_metrics_mod3,test_metrics_mod4,test_metrics_mod5,test_metrics_mod6,
                              test_metrics_mod7,test_metrics_mod8,test_metrics_mod9,test_metrics_mod10)
test_metrics_all <- test_metrics_all %>% mutate(model_number = 1:10,
                                                type = "test",
                                                ID = 11:20)

test_metrics_all
```

```{r}
metrics_all <- bind_rows(train_metrics_all,test_metrics_all)

metrics_all %>% ggplot(mapping = aes(x = model_number, y = rmse_value)) +
  geom_line(mapping = aes(color = type,
                          group = type),
            size = 1.1) +
  geom_point(mapping = aes(color = type),
             size = 2.5) +
  labs(x = 'model') +
  theme_bw()
```

```{r}
metrics_all %>% ggplot(mapping = aes(x = model_number, y = r2_value)) +
  geom_line(mapping = aes(color = type,
                          group = type),
            size = 1.1) +
  geom_point(mapping = aes(color = type),
             size = 2.5) +
  labs(x = 'model') +
  theme_bw()
```


Model 5
```{r}
mod5_lm %>%
  coefplot(scales = "free", sort = "magnitude", pointSize = 1, innerCI = 0.5)+
  geom_vline(xintercept = 0, color = "red")+
  theme_bw()+
  theme(legend.position = 'none')
```

```{r}
df_5_lm <- mod5_lm %>%
        coefplot(sort = "magnitude", plot = FALSE) %>%
        tibble::as_tibble() %>%
        filter((HighOuter > 0 & LowOuter > 0) | (HighOuter < 0 & LowOuter < 0))

df_5_lm %>% head()

```

Model 8 
```{r}
mod8_lm %>%
  coefplot(scales = "free", sort = "magnitude", pointSize = 1, innerCI = 0.5)+
  geom_vline(xintercept = 0, color = "red")+
  theme_bw()+
  theme(legend.position = 'none')
```

```{r}
df_8_lm <- mod8_lm %>%
        coefplot(sort = "magnitude", plot = FALSE) %>%
        tibble::as_tibble() %>%
        filter((HighOuter > 0 & LowOuter > 0) | (HighOuter < 0 & LowOuter < 0))

df_8_lm %>% head()
```


Model 10
```{r}
mod10_lm %>%
  coefplot(scales = "free", sort = "magnitude", pointSize = 1, innerCI = 0.5)+
  geom_vline(xintercept = 0, color = "red")+
  theme_bw()+
  theme(legend.position = 'none')
```

```{r}
df_10_lm <- mod10_lm %>%
        coefplot(sort = "magnitude", plot = FALSE) %>%
        tibble::as_tibble() %>%
        filter((HighOuter > 0 & LowOuter > 0) | (HighOuter < 0 & LowOuter < 0))

df_10_lm %>% head()
```







