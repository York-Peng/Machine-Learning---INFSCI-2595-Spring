---
title: "Final_Project_Part_3_A"
author: "Peng Yuan"
date: '2023-04-20'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Classification – iiiA) GLM
### 1. Load package and Data
#### 1.1 Load package
```{r}
library(tidyverse)
library(caret)
library(coefplot)
library(rstanarm)
library(splines)
library(kernlab)
```

#### 1.2 Load data
```{r}
df_all <- readr::read_csv("paint_project_train_data.csv", col_names = TRUE)

df <- df_all %>%
  subset(select = c(R, G, B, Hue)) %>% 
  scale() %>% as.data.frame() %>%
  bind_cols(df_all %>% subset(select = c(Lightness, Saturation, outcome)))

df %>% glimpse()
```

#### 1.3 Resampling
```{r}
set.seed(4321)

trainIndex <- sample(1:nrow(df), round(nrow(df)*0.2))

train <- df[trainIndex, ]
test <- df[-trainIndex, ]
```


### 2. Build Model
#### 2.1 Intercept-only model – no INPUTS!
```{r,2.1}
mod1_glm <- glm(outcome ~ 1 , train,family = "binomial")
```

#### 2.2 Categorical variables only – linear additive
```{r,2.2}
mod2_glm <- glm(outcome ~ Saturation + Lightness , train,family = "binomial")
```


#### 2.3 Continuous variables only – linear additive
```{r,2.3}
mod3_glm <- glm(outcome ~ R + G + B + Hue, train,family = "binomial")
```

#### 2.4 All categorical and continuous variables – linear additive
```{r,2.4}
mod4_glm <- glm(outcome ~  R + G + B + Hue + Saturation + Lightness, train,family = "binomial")
```

#### 2.5 Interaction of the categorical inputs with all continuous inputs main effects
```{r,2.5}
mod5_glm <- glm(outcome ~ (R + G + B + Hue) * (Saturation + Lightness), train,family = "binomial")
```

#### 2.6 Add categorical inputs to all main effect and all pairwise interactions of continuous inputs
```{r,2.6}
mod6_glm <- glm(outcome ~ (R + G + B + Hue)^2 + (Saturation + Lightness), data = train,family = "binomial")
```

#### 2.7 Interaction of the categorical inputs with all main effect and all pairwise interactions of continuous inputs
```{r,2.7}
mod7_glm <- glm(outcome ~ (R + G + B + Hue)^2 + (Saturation * Lightness), train,family = "binomial")
```



#### 2.8 3 models with basis functions of your choice
• Try non-linear basis functions based on your EDA.
• Can consider interactions of basis functions with other basis functions!
• Can consider interactions of basis functions with the categorical inputs!
```{r,2.8}
mod8_glm <- glm(outcome ~ (I(R^2) + I(G^2) + I(B^2) + I(Hue^2)) + (Saturation + Lightness), train,family = "binomial")

mod9_glm <- glm(outcome ~ (I(R^2) + I(G^2) + I(B^2) + I(Hue^2)) * (Saturation + Lightness), train,family = "binomial")

mod10_glm <- glm(outcome ~ (I(R^2) + I(G^2) + I(B^2) + I(Hue^2) + R * G * B * Hue) + (Saturation * Lightness), train,family = "binomial")
```

### 3. Model Evaluation
#### 3.1 R- square
```{r,3.1}
p1 <- broom::glance(mod1_glm)
p2 <- broom::glance(mod2_glm)
p3 <- broom::glance(mod3_glm)
p4 <- broom::glance(mod4_glm)
p5 <- broom::glance(mod5_glm)
p6 <- broom::glance(mod6_glm)
p7 <- broom::glance(mod7_glm)
p8 <- broom::glance(mod8_glm)
p9 <- broom::glance(mod9_glm)
p10 <- broom::glance(mod10_glm)

p_all <- rbind(p1,p2,p3,p4,p5,p6,p7,p8,p9,p10) %>%
  tibble::rowid_to_column()

p_all
```

```{r}
p_all %>% 
  ggplot(mapping = aes(x = c(1:10), y = deviance, group = 1))+
  geom_path()+
  geom_point(size = 2.0)+
  theme_bw()
```

#### 3.2 AIC & BIC
```{r,3.2_AIC}
p_all %>%
  ggplot(mapping = aes(x = c(1:10), y = AIC, group = 1))+
  geom_path()+
  geom_point(size = 2.0)+
  theme_bw()
```

```{r,3.2_BIC}
p_all %>%
  ggplot(mapping = aes(x = c(1:10), y = BIC, group = 1))+
  geom_path()+
  geom_point(size = 2.0)+
  theme_bw()
```


#### 3.3 Accuracy & Specificity & Sensitivity & Precision
```{r,model1}
mod1_predict <- predict(mod1_glm, test, type = "response")
mod1_predict_class <- ifelse(mod1_predict > 0.50, 1, 0)
matrix_mod1 <- confusionMatrix(as.factor(mod1_predict_class), as.factor(test$outcome))
matrix_mod1
```


```{r,model2}
mod2_predict <- predict(mod2_glm, test, type = "response")
mod2_predict_class <- ifelse(mod2_predict > 0.50, 1, 0)
matrix_mod2 <- confusionMatrix(as.factor(mod2_predict_class), as.factor(test$outcome))
matrix_mod2
```


```{r,model3}
mod3_predict <- predict(mod3_glm, test, type = "response")
mod3_predict_class <- ifelse(mod3_predict > 0.50, 1, 0)
matrix_mod3 <- confusionMatrix(as.factor(mod3_predict_class), as.factor(test$outcome))
matrix_mod3
```

```{r,model4}
mod4_predict <- predict(mod4_glm, test, type = "response")
mod4_predict_class <- ifelse(mod4_predict > 0.50, 1, 0)
matrix_mod4 <- confusionMatrix(as.factor(mod4_predict_class), as.factor(test$outcome))
matrix_mod4
```

```{r,model5}
mod5_predict <- predict(mod5_glm, test, type = "response")
mod5_predict_class <- ifelse(mod5_predict > 0.50, 1, 0)
matrix_mod5 <- confusionMatrix(as.factor(mod5_predict_class), as.factor(test$outcome))
matrix_mod5
```


```{r,model6}
mod6_predict <- predict(mod6_glm, test, type = "response")
mod6_predict_class <- ifelse(mod6_predict > 0.50, 1, 0)
matrix_mod6 <- confusionMatrix(as.factor(mod6_predict_class), as.factor(test$outcome))
matrix_mod6
```

```{r,model7}
mod7_predict <- predict(mod7_glm, test, type = "response")
mod7_predict_class <- ifelse(mod7_predict > 0.50, 1, 0)
matrix_mod7 <- confusionMatrix(as.factor(mod7_predict_class), as.factor(test$outcome))
matrix_mod7
```

```{r}
mod8_predict <- predict(mod8_glm, test, type = "response")
mod8_predict_class <- ifelse(mod8_predict > 0.50, 1, 0)
matrix_mod8 <- confusionMatrix(as.factor(mod8_predict_class), as.factor(test$outcome))
matrix_mod8
```


```{r}
mod9_predict <- predict(mod9_glm, test, type = "response")
mod9_predict_class <- ifelse(mod9_predict > 0.50, 1, 0)
matrix_mod9 <- confusionMatrix(as.factor(mod9_predict_class), as.factor(test$outcome))
matrix_mod9
```



```{r}
mod10_predict <- predict(mod10_glm, test, type = "response")
mod10_predict_class <- ifelse(mod10_predict > 0.50, 1, 0)
matrix_mod10 <- confusionMatrix(as.factor(mod10_predict_class), as.factor(test$outcome))
matrix_mod10
```

```{r,Accurancy}
perf_overall <- bind_rows(matrix_mod1$overall, 
                      matrix_mod2$overall, 
                      matrix_mod3$overall, 
                      matrix_mod4$overall, 
                      matrix_mod5$overall, 
                      matrix_mod6$overall,
                      matrix_mod7$overall,
                      matrix_mod8$overall,
                      matrix_mod9$overall,
                      matrix_mod10$overall)

perf_overall %>%
  ggplot(mapping = aes(x = c(1:10), y = Accuracy))+
  geom_path()+
  geom_point(size = 2.0)
  theme_bw()
```

```{r,Sensitivity}
perf_byClass <- bind_rows(matrix_mod1$byClass, 
                      matrix_mod2$byClass, 
                      matrix_mod3$byClass, 
                      matrix_mod4$byClass, 
                      matrix_mod5$byClass, 
                      matrix_mod6$byClass,
                      matrix_mod7$byClass,
                      matrix_mod8$byClass,
                      matrix_mod9$byClass,
                      matrix_mod10$byClass)
```

```{r}
perf_byClass %>%
  ggplot(mapping = aes(x = c(1:10)))+
  geom_point(mapping = aes(y = Sensitivity),size = 3.0,color = "blue") +
  geom_point(mapping = aes(y = Specificity),size = 3.0, color = "red") +
  geom_path(mapping = aes(y = Sensitivity),,size = 2.0,color = "blue")+
  geom_path(mapping = aes(y = Specificity),,size = 2.0,color = "red") +
  theme_bw()
```



```{r,Precision}
perf_byClass %>%
  ggplot(mapping = aes(x = c(1:10), y = Precision))+
  geom_path()+
  geom_point(size = 2.0)
  theme_bw()
```

Taking sensitivity, specificity, accuracy, AIC and BIC into account, I think model 8 is the best model and model 6 is the second best.

```{r}
mod8_glm %>% readr::write_rds("Model/calss_glm_mod8.rds")
mod6_glm %>% readr::write_rds("Model/calss_glm_mod6.rds")
```


### 4. Coefficient summary - TOP 3 Model
#### Model 6
```{r}
mod6_glm %>%
  coefplot(scales = "free", sort = "natural", pointSize = 1, innerCI = 0.5)+
  geom_vline(xintercept = 0, color = "red")+
  theme_bw()+
  theme(legend.position = 'none')
```

```{r}
import_mod6 <- mod6_glm %>% 
  coefplot(sort = "magnitude", plot = FALSE) %>%
  tibble::as_tibble() %>%
  filter(LowOuter > 0 | HighOuter < 0)

import_mod6
```

There are 7 significant coefficient features in Model 6 and the R:G is the most important.


#### Model 8
```{r}
mod8_glm %>%
  coefplot(scales = "free", sort = "natural", pointSize = 1, innerCI = 0.5)+
  geom_vline(xintercept = 0, color = "red")+
  theme_bw()+
  theme(legend.position = 'none')
```

```{r}
import_mod8 <- mod8_glm %>% 
  coefplot(sort = "magnitude", plot = FALSE) %>%
  tibble::as_tibble() %>%
  filter(LowOuter > 0 | HighOuter < 0 )

import_mod8
```

There are 9 significant coefficient features in Model 8 and the I(G^2) is the most important.

#### Model 4
```{r}
mod4_glm %>%
  coefplot(scales = "free", sort = "natural", pointSize = 1, innerCI = 0.5)+
  geom_vline(xintercept = 0, color = "red")+
  theme_bw()+
  theme(legend.position = 'none')
```

```{r}
import_mod4 <- mod4_glm %>% 
  coefplot(sort = "magnitude", plot = FALSE) %>%
  tibble::as_tibble() %>%
  filter(LowOuter > 0 | HighOuter < 0)

import_mod4
```

There are 4 significant coefficient features in Model 4 and the G is the most important.







